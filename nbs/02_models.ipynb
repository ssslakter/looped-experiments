{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../configs\", version_base=None):\n",
    "    cfg = compose(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb:\n",
      "  project: looped-transformer\n",
      "  log_every_steps: 100\n",
      "gpu:\n",
      "  cuda: true\n",
      "model:\n",
      "  family: gpt2\n",
      "  n_embd: 256\n",
      "  n_layer: 12\n",
      "  n_head: 8\n",
      "  n_dims: 20\n",
      "  n_positions: 101\n",
      "  dropout: 0.0\n",
      "  bias: true\n",
      "task_name: linear_regression\n",
      "task:\n",
      "  n_points: 30\n",
      "  n_dim: 20\n",
      "  std: 0.1\n",
      "  sparsity: null\n",
      "training:\n",
      "  batch_size: 64\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.0\n",
      "  train_steps: 500000\n",
      "  save_every_steps: 1000\n",
      "  keep_every_steps: 100000\n",
      "  curriculum:\n",
      "    dims:\n",
      "      start: 5\n",
      "      end: 20\n",
      "      inc: 1\n",
      "      interval: 5000\n",
      "    points:\n",
      "      start: 11\n",
      "      end: 41\n",
      "      inc: 2\n",
      "      interval: 5000\n",
      "    loops:\n",
      "      start: 1\n",
      "      end: 1\n",
      "      inc: 2\n",
      "      interval: 500\n",
      "  n_loop_window: 20\n",
      "out_dir: ./results2/linear_regression_baseline\n",
      "debug_mode: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import math\n",
    "from looped_experiments.nano_gpt import Block, LayerNorm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerBase(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        block_size = cfg.n_positions * 2 + 1  # input, output pairs + 1 for the target\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wpe=nn.Embedding(block_size, cfg.n_embd),\n",
    "            drop=nn.Dropout(cfg.dropout),\n",
    "            h=nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)]),\n",
    "            ln_f=LayerNorm(cfg.n_embd, bias=cfg.bias),\n",
    "        ))\n",
    "        if self.__class__ == TransformerBase:\n",
    "            self._init_all_params(cfg.n_layer)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f\"Initializing {module}\")\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f\"Initializing emb {module}\")\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def _init_all_params(self):\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * self.cfg.n_layer))\n",
    "\n",
    "    def forward(self, embs):\n",
    "        device = embs.device\n",
    "        _, t = embs.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(embs + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(TransformerBase):\n",
    "    '''Transformer for tasks from in-context learning'''\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        self.read_in = nn.Linear(cfg.n_dims, cfg.n_embd)\n",
    "        self.read_out = nn.Linear(cfg.n_embd, 1)\n",
    "\n",
    "        self._init_all_params()\n",
    "\n",
    "    def create_prompt(self, xs, ys):\n",
    "        n_dim = xs.shape[-1]\n",
    "        y_wide = F.pad(ys.unsqueeze(-1), (0, n_dim - 1), value=0)\n",
    "        return torch.stack((xs, y_wide), dim=2).flatten(1, 2)[0]\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        x = self.create_prompt(xs, ys)\n",
    "        x = self.read_in(x)\n",
    "        x = super().forward(x)\n",
    "        y = self.read_out(x)\n",
    "        # y = y[:, self.ind::self.freq, 0] #TODO understand what is this\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looped_experiments.tasks import LinearRegression, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing emb Embedding(203, 256)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=768, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1024, bias=True)\n",
      "Initializing Linear(in_features=1024, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=20, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(cfg.model)\n",
    "dl = dataloader(LinearRegression(cfg.training.batch_size, **cfg.task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
