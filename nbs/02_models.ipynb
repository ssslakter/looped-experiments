{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from looped_experiments.nano_gpt import Block, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class MaskedTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            drop=nn.Dropout(cfg.dropout),\n",
    "            h=nn.ModuleList([Block(cfg) for _ in range(cfg.n_layer)]),\n",
    "        ))\n",
    "\n",
    "    def forward(self, embs):\n",
    "        x = self.transformer.drop(embs)\n",
    "        for block in self.transformer.h: x = block(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "def init_all_params(model):\n",
    "    # init all weights\n",
    "    model.apply(init_weights)\n",
    "    # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "    for pn, p in model.named_parameters():\n",
    "        if pn.endswith('c_proj.weight'):\n",
    "            torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * model.cfg.n_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Transformer(nn.Module):\n",
    "    '''Transformer for tasks from in-context learning'''\n",
    "\n",
    "    def __init__(self, cfg, freq=2):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.freq = freq  # one input, one output, and so on\n",
    "        self.backbone = MaskedTransformer(cfg)\n",
    "        self.block_size = cfg.n_positions * freq + 1  # input, output pairs + 1 for the target\n",
    "        self.read_in = nn.Linear(cfg.n_dims, cfg.n_embd)\n",
    "        self.wpe=nn.Embedding(self.block_size, cfg.n_embd)\n",
    "        \n",
    "        self.ln_f=LayerNorm(cfg.n_embd, bias=cfg.bias)\n",
    "        self.read_out = nn.Linear(cfg.n_embd, 1)\n",
    "        init_all_params(self)\n",
    "\n",
    "    def create_prompt(self, xs, ys):\n",
    "        n_dims = xs.shape[-1]\n",
    "        y_wide = F.pad(ys.unsqueeze(-1), (0, n_dims - 1), value=0)\n",
    "        return torch.stack((xs, y_wide), dim=2).flatten(1, 2)\n",
    "    \n",
    "    def add_positional(self, embs):\n",
    "        device = embs.device\n",
    "        _, t, _ = embs.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        return embs + self.wpe(pos)\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        x = self.create_prompt(xs, ys)\n",
    "        x = self.read_in(x)\n",
    "        x = self.add_positional(x)\n",
    "        x = self.backbone(x)\n",
    "        x = self.ln_f(x)\n",
    "        y = self.read_out(x).squeeze(-1)\n",
    "        y = y[:, ::self.freq]  # only take the outputs (every other element)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LoopedTransformer(Transformer):\n",
    "    '''Looped transformer for tasks from in-context learning'''\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, freq=2)\n",
    "        self.n_loops, self.n_loop_window = cfg.n_loops, cfg.n_loop_window\n",
    "        self.repeat_positional = cfg.repeat_positional\n",
    "        self.repeat_ln = cfg.repeat_ln\n",
    "        self.keep_n_tokens = cfg.keep_n_tokens\n",
    "        self.token_dec = cfg.token_dec or 0\n",
    "\n",
    "    def _model(self, z, x):\n",
    "        z = z + x  # residual with addition\n",
    "        if self.repeat_positional: z = self.add_positional(z)\n",
    "        z = self.backbone(z)\n",
    "        if self.repeat_ln: z = self.ln_f(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        horizon_start = max(0, self.n_loops - self.n_loop_window)\n",
    "        x = self.create_prompt(xs, ys)\n",
    "        b, t, _ = x.size()\n",
    "        x = self.read_in(x)\n",
    "        if not self.repeat_positional: x = self.add_positional(x)\n",
    "        z = torch.zeros_like(x)\n",
    "        keep = t\n",
    "        preds = torch.zeros(self.n_loop_window, b, t // self.freq, device=x.device) + float('nan')\n",
    "        for i in range(self.n_loops):\n",
    "            if i < horizon_start:\n",
    "                with torch.no_grad(): z = self._model(z, x)\n",
    "                continue\n",
    "            z = self._model(z, x)\n",
    "            y = z if self.repeat_ln else self.ln_f(z)\n",
    "            y = self.read_out(y).squeeze(-1)\n",
    "            preds[i - horizon_start, :, -(x.size(1) // self.freq):] = y[:, (x.size(1) - 2) % self.freq::self.freq]\n",
    "            # remove first tokens\n",
    "            keep = max(self.keep_n_tokens or t, keep - self.token_dec)\n",
    "            z = z[:, -keep:]\n",
    "            x = x[:, -keep:]\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "def loop_loss(preds, ys):\n",
    "    return torch.nanmean((preds - ys.expand_as(preds)).square())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_model(cfg):\n",
    "    models = {\n",
    "        'gpt2': Transformer,\n",
    "        'gpt2_loop': LoopedTransformer\n",
    "    }\n",
    "    return models[cfg.family](cfg)\n",
    "\n",
    "def get_loss(cfg):\n",
    "    losses = {\n",
    "        'gpt2': F.mse_loss,\n",
    "        'gpt2_loop': loop_loss\n",
    "    }\n",
    "    return losses[cfg.family]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looped_experiments.tasks import LinearRegression\n",
    "from looped_experiments.utils import get_config, show_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config(overrides=['model=loop', 'model.token_dec=2', 'model.keep_n_tokens=5'])\n",
    "# show_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LoopedTransformer(cfg.model).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys = LinearRegression(64,**cfg.task)()\n",
    "xs, ys = xs.to('cuda'), ys.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5480, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop_loss(res, ys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
