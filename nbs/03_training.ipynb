{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            o.callback(f'before_{self.nm}')\n",
    "            f(o, *args, **kwargs)\n",
    "            o.callback(f'after_{self.nm}')\n",
    "        return _f\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, dl_train, dl_valid, n_epoch, optimizer=optim.Adam, loss_fn=F.mse_loss, lr=1e-3, wd=0.0, cbs=None):\n",
    "        self.model, self.dl_train, self.dl_valid, self.n_epoch, self.loss_fn, self.cbs \\\n",
    "            = model, dl_train, dl_valid, n_epoch, loss_fn, cbs\n",
    "        self.optimizer = optimizer(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    @with_cbs(\"fit\")\n",
    "    def fit(self, lr=None, n_epoch=None):\n",
    "        # update params if provided\n",
    "        if lr is not None: self.optimizer.param_groups[0]['lr'] = lr\n",
    "        n_epoch = n_epoch or self.n_epoch\n",
    "\n",
    "        self.train_step = 0\n",
    "        self.mb = master_bar(range(n_epoch))\n",
    "        for i in self.mb:\n",
    "            self.one_epoch(True)\n",
    "            self.mb.write(f\"Epoch {i} Loss: {self.loss.item()}\")\n",
    "            torch.no_grad()(self.one_epoch)(False)\n",
    "            self.mb.write(f\"Valid Loss: {self.loss.item()}\")\n",
    "            \n",
    "    def predict(self, dl=None):\n",
    "        dl = dl or self.dl_valid\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        for b in progress_bar(dl, total=len(dl)):\n",
    "            xb, _ = b\n",
    "            preds.append(self.model(*xb).detach().cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        dl = self.dl_train if training else self.dl_valid\n",
    "        for b in progress_bar(dl, parent=self.mb, total=len(dl)):\n",
    "            self.xb, self.yb = b\n",
    "            self.one_batch()\n",
    "            self.mb.child.comment = f\"Loss: {self.loss.item()}\"\n",
    "\n",
    "    @with_cbs(\"batch\")\n",
    "    def one_batch(self):\n",
    "        y_hat = self.model(*self.xb)\n",
    "        self.loss = self.loss_fn(y_hat, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.train_step += 1\n",
    "\n",
    "    def callback(self, nm):\n",
    "        if self.cbs is not None:\n",
    "            for cb in self.cbs:\n",
    "                if hasattr(cb, nm): getattr(cb, nm)(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def default_device():\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    return torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "class ToDeviceCB:\n",
    "    def __init__(self, device=default_device()): self.device = device\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        self.model = learn.model.to(self.device)\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        learn.xb, learn.yb = learn.xb.to(self.device), learn.yb.to(self.device)\n",
    "\n",
    "\n",
    "class FnCallback:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "\n",
    "    def __call__(self, fn):\n",
    "        setattr(self, self.nm, fn)\n",
    "        return self\n",
    "\n",
    "\n",
    "class SaveModelCB:\n",
    "    def __init__(self, save_dir, save_every_n=1000):\n",
    "        self.dir, self.save_every_n = Path(save_dir), save_every_n\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.train_step % self.save_every_n == 0:\n",
    "            torch.save(learn.model.state_dict(), self.dir / f\"model_{learn.train_step}.pt\")\n",
    "\n",
    "    def after_fit(self, learn):\n",
    "        torch.save(learn.model.state_dict(), self.dir / f\"model_last.pt\")\n",
    "\n",
    "\n",
    "class WandbCB:\n",
    "    def __init__(self, config):\n",
    "        try: import wandb; wandb.require(\"core\")\n",
    "        except ImportError: raise ImportError(\"Please install wandb to use this callback\")\n",
    "        self.wandb = wandb\n",
    "        self.cfg = config\n",
    "        self.wcfg = config.wandb\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        learn.stats = {}\n",
    "        self.run = self.wandb.init(project=self.wcfg.project,\n",
    "                        mode=\"disabled\" if self.cfg.debug_mode else \"online\")\n",
    "        self.run.config = self.cfg\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.train_step % self.wcfg.log_every_steps == 0:\n",
    "            self.run.log(\n",
    "                {\n",
    "                    \"loss\": learn.loss,\n",
    "                    \"lr\": learn.optimizer.param_groups[0]['lr'],\n",
    "                    **learn.stats\n",
    "                },\n",
    "                step=learn.train_step,\n",
    "            )\n",
    "\n",
    "    def after_fit(self, _): self.run.finish()\n",
    "    \n",
    "\n",
    "DEFAULT_CBS = [ToDeviceCB(), SaveModelCB(\"models\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class CurriculumCB:\n",
    "    def __init__(self, curriculum_config):\n",
    "        self.cfg = curriculum_config\n",
    "\n",
    "    def get_params(self, name, step):\n",
    "        cfg = self.cfg[name]\n",
    "        return min(cfg.end, cfg.start + step // cfg.interval * cfg.inc)\n",
    "    \n",
    "    def update_task(self, learn, ds):\n",
    "        n_dims = self.get_params(\"dims\", learn.train_step)\n",
    "        ds.task.truncated_dims = ds.task.n_dim - n_dims\n",
    "        ds.task.n_points = self.get_params(\"points\", learn.train_step)\n",
    "        if hasattr(learn, 'stats'):\n",
    "            learn.stats['n_dims'] = n_dims\n",
    "            learn.stats['n_points'] = ds.task.n_points\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.model.training: self.update_task(learn, learn.dl_train.dataset)\n",
    "        else: self.update_task(learn, learn.dl_valid.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize\n",
    "\n",
    "from looped_experiments.models import Transformer\n",
    "from looped_experiments.tasks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../configs\", version_base=None):\n",
    "    cfg = compose(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cfg.training\n",
    "task = LinearRegression(train.batch_size, **cfg.task)\n",
    "dl_train = dataloader(task, train.train_steps)\n",
    "dl_eval = dataloader(task, train.eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = Transformer(cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@FnCallback(\"before_batch\")\n",
    "def trans_input(learner):  learner.xb = (learner.xb, learner.yb)\n",
    "\n",
    "cbs = [WandbCB(cfg), ToDeviceCB(), CurriculumCB(cfg.training.curriculum), trans_input]\n",
    "learn = Learner(model, dl_train, dl_eval, cfg.training.n_epoch, cbs=cbs)\n",
    "learn.fit(lr=cfg.training.learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
