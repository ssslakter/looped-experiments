{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from omegaconf import OmegaConf\n",
    "from torch import optim\n",
    "\n",
    "from looped_experiments.utils import def_device, to_cpu, to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            o.callback(f'before_{self.nm}')\n",
    "            f(o, *args, **kwargs)\n",
    "            o.callback(f'after_{self.nm}')\n",
    "        return _f\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self, model, dl_train, dl_valid, n_epoch, optimizer=optim.Adam, loss_fn=F.mse_loss, lr=1e-3, wd=0.0, cbs=None):\n",
    "        self.model, self.dl_train, self.dl_valid, self.n_epoch, self.loss_fn, self.cbs \\\n",
    "            = model, dl_train, dl_valid, n_epoch, loss_fn, cbs\n",
    "        self.optimizer = optimizer(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        self.mb = None\n",
    "\n",
    "    @with_cbs(\"fit\")\n",
    "    def fit(self, lr=None, n_epoch=None):\n",
    "        # update params if provided\n",
    "        if lr is not None: self.optimizer.param_groups[0]['lr'] = lr\n",
    "        n_epoch = n_epoch or self.n_epoch\n",
    "\n",
    "        self.train_step = 0\n",
    "        self.mb = master_bar(range(n_epoch))\n",
    "        for i in self.mb:\n",
    "            self.one_epoch(True, self.dl_train)\n",
    "            self.mb.write(f\"Epoch {i} Loss: {self.loss.item()}\")\n",
    "            torch.no_grad()(self.one_epoch)(False, self.dl_valid)\n",
    "            self.mb.write(f\"Valid Loss: {self.loss.item()}\")\n",
    "    \n",
    "    @with_cbs(\"fit\")\n",
    "    def eval(self, dl=None):\n",
    "        dl = dl or self.dl_valid\n",
    "        torch.no_grad()(self.one_epoch)(False, dl)\n",
    "    \n",
    "    @with_cbs(\"epoch\")\n",
    "    def one_epoch(self, training, dl):\n",
    "        self.model.train(training)\n",
    "        for b in progress_bar(dl, parent=self.mb, total=len(dl)):\n",
    "            self.xb, self.yb = b\n",
    "            self.one_batch()\n",
    "            if self.mb:\n",
    "                self.mb.child.comment = f\"Loss: {self.loss.item()}\"\n",
    "\n",
    "    @with_cbs(\"batch\")\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(*self.xb)\n",
    "        self.loss = self.loss_fn(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.train_step += 1\n",
    "\n",
    "    def callback(self, nm):\n",
    "        if self.cbs is not None:\n",
    "            for cb in sorted(self.cbs):\n",
    "                if hasattr(cb, nm): getattr(cb, nm)(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Callback:\n",
    "    order = 0\n",
    "    def __gt__(self, other): return self.order > other.order\n",
    "\n",
    "\n",
    "class ToDeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): self.device = device\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        self.model = learn.model.to(self.device)\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        learn.xb, learn.yb = to_device(learn.xb, self.device), to_device(learn.yb, self.device)\n",
    "\n",
    "\n",
    "class FnCallback(Callback):\n",
    "    order = 1\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "\n",
    "    def __call__(self, fn):\n",
    "        setattr(self, self.nm, fn)\n",
    "        return self\n",
    "\n",
    "\n",
    "class SaveModelCB(Callback):\n",
    "    order = 1\n",
    "\n",
    "    def __init__(self, save_dir, save_every_n=1000, max_to_keep=5, save_training=True):\n",
    "        self.dir, self.save_every_n, self.max_to_keep = Path(save_dir), save_every_n, max_to_keep\n",
    "        self.save_training = save_training\n",
    "        self.dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.train_step % self.save_every_n == 0:\n",
    "            if self.save_training:\n",
    "                torch.save({\n",
    "                    'step': learn.train_step,\n",
    "                    'model_state_dict': learn.model.state_dict(),\n",
    "                    'optimizer_state_dict': learn.optimizer.state_dict()},\n",
    "                    self.dir / f\"model_{learn.train_step}+train.pt\")\n",
    "            else: torch.save(learn.model.state_dict(), self.dir / f\"model_{learn.train_step}.pt\")\n",
    "        files = sorted(self.dir.glob(\"model_*.pt\"), key=lambda x: x.stem.split(\"_\")[1], reverse=True)\n",
    "        if len(files) > self.max_to_keep:\n",
    "            for f in files[self.max_to_keep:]: f.unlink()\n",
    "\n",
    "    def after_fit(self, learn):\n",
    "        torch.save(learn.model.state_dict(), self.dir / f\"model_last.pt\")\n",
    "\n",
    "\n",
    "class TimerCB(Callback):\n",
    "    def before_fit(self, learn):\n",
    "        if not hasattr(learn, \"stats\"):\n",
    "            learn.stats = {}\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        if learn.model.training:\n",
    "            self.start = time.time()\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.model.training:\n",
    "            end = time.time()\n",
    "            learn.stats['steps_per_s'] = 1 / (end - self.start)\n",
    "\n",
    "\n",
    "class WandbCB(Callback):\n",
    "    order = SaveModelCB.order + 10\n",
    "\n",
    "    def __init__(self, config):\n",
    "        try: import wandb; wandb.require(\"core\")\n",
    "        except ImportError: raise ImportError(\"Please install wandb to use this callback\")\n",
    "        self.wandb = wandb\n",
    "        self.cfg = config\n",
    "        self.wcfg = config.wandb\n",
    "\n",
    "    def before_fit(self, learn):\n",
    "        learn.stats = {}\n",
    "        self.run = self.wandb.init(project=self.wcfg.project,\n",
    "                                   name=self.wcfg.name,\n",
    "                                   config=OmegaConf.to_container(self.cfg, resolve=True),\n",
    "                                   mode=\"disabled\" if self.cfg.debug_mode else \"online\")\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.train_step % self.wcfg.log_every_steps == 0:\n",
    "            if learn.model.training:\n",
    "                self.run.log({\n",
    "                    \"loss\": learn.loss,\n",
    "                    \"lr\": learn.optimizer.param_groups[0]['lr'],\n",
    "                    **learn.stats\n",
    "                }, step=learn.train_step)\n",
    "            else:\n",
    "                self.run.log({\"valid_loss\": learn.loss}, step=learn.train_step)\n",
    "\n",
    "    def after_fit(self, learn):\n",
    "        for cb in learn.cbs:\n",
    "            if isinstance(cb, SaveModelCB):\n",
    "                self.run.save(str(cb.dir / \"model_last.pt\"))\n",
    "        self.run.finish()\n",
    "\n",
    "\n",
    "def repr_cbs(cbs):\n",
    "    return \" \".join(cb.__class__.__name__ for cb in cbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curriculum for tasks and loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumCB(Callback):\n",
    "    def __init__(self, curriculum_config):\n",
    "        self.cfg = curriculum_config\n",
    "\n",
    "    def get_params(self, name, step):\n",
    "        cfg = self.cfg[name]\n",
    "        return min(cfg.end, cfg.start + step // cfg.interval * cfg.inc)\n",
    "\n",
    "    def update_task(self, learn, ds):\n",
    "        ds.task.n_points = self.get_params(\"points\", learn.train_step)\n",
    "        n_dims = self.get_params(\"dims\", learn.train_step)\n",
    "        ds.task.truncated_dims = ds.task.n_dims - n_dims\n",
    "        if hasattr(learn, 'stats'):\n",
    "            learn.stats['n_points'] = ds.task.n_points\n",
    "            learn.stats['n_dims'] = n_dims\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if learn.model.training: self.update_task(learn, learn.dl_train.dataset)\n",
    "        else: self.update_task(learn, learn.dl_valid.dataset)\n",
    "\n",
    "\n",
    "class LoopCB(CurriculumCB):\n",
    "    def update_model(self, learn):\n",
    "        learn.model.n_loops = self.get_params(\"loops\", learn.train_step)\n",
    "        if hasattr(learn, 'stats'):\n",
    "            learn.stats['n_loops'] = learn.model.n_loops\n",
    "\n",
    "    def after_batch(self, learn): self.update_model(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from looped_experiments.models import get_loss, get_model\n",
    "from looped_experiments.tasks import LinearRegression, dataloader\n",
    "from looped_experiments.utils import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config(overrides=[\"model=loop\", \"training=loop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cfg.training\n",
    "task = LinearRegression(train.batch_size, **cfg.task)\n",
    "dl_train = dataloader(task, train.train_steps)\n",
    "dl_eval = dataloader(task, train.eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@FnCallback(\"before_batch\")\n",
    "def trans_input(learner): learner.xb = (learner.xb, learner.yb)\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model = get_model(cfg.model)\n",
    "cbs = [ToDeviceCB(), CurriculumCB(cfg.training.curriculum), trans_input, LoopCB(cfg.model.curriculum)]\n",
    "learn = Learner(model, dl_train, dl_eval, cfg.training.n_epoch, loss_fn=get_loss(cfg.model), cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr=cfg.training.learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
