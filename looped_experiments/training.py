# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_training.ipynb.

# %% auto 0
__all__ = ['with_cbs', 'Learner', 'default_device', 'Callback', 'ToDeviceCB', 'FnCallback', 'SaveModelCB', 'WandbCB', 'repr_cbs',
           'CurriculumCB', 'LoopCB']

# %% ../nbs/03_training.ipynb 1
from pathlib import Path

import torch
import torch.nn.functional as F
from fastprogress import master_bar, progress_bar
from omegaconf import OmegaConf
from torch import optim

# %% ../nbs/03_training.ipynb 2
class with_cbs:
    def __init__(self, nm): self.nm = nm

    def __call__(self, f):
        def _f(o, *args, **kwargs):
            o.callback(f'before_{self.nm}')
            f(o, *args, **kwargs)
            o.callback(f'after_{self.nm}')
        return _f


class Learner:
    def __init__(self, model, dl_train, dl_valid, n_epoch, optimizer=optim.Adam, loss_fn=F.mse_loss, lr=1e-3, wd=0.0, cbs=None):
        self.model, self.dl_train, self.dl_valid, self.n_epoch, self.loss_fn, self.cbs \
            = model, dl_train, dl_valid, n_epoch, loss_fn, cbs
        self.optimizer = optimizer(model.parameters(), lr=lr, weight_decay=wd)

    @with_cbs("fit")
    def fit(self, lr=None, n_epoch=None):
        # update params if provided
        if lr is not None: self.optimizer.param_groups[0]['lr'] = lr
        n_epoch = n_epoch or self.n_epoch

        self.train_step = 0
        self.mb = master_bar(range(n_epoch))
        for i in self.mb:
            self.one_epoch(True)
            self.mb.write(f"Epoch {i} Loss: {self.loss.item()}")
            torch.no_grad()(self.one_epoch)(False)
            self.mb.write(f"Valid Loss: {self.loss.item()}")

    def predict(self, dl=None):
        dl = dl or self.dl_valid
        self.model.eval()
        preds = []
        for b in progress_bar(dl, total=len(dl)):
            xb, _ = b
            preds.append(self.model(*xb).detach().cpu())
        return torch.cat(preds)

    def one_epoch(self, training):
        self.model.train(training)
        dl = self.dl_train if training else self.dl_valid
        for b in progress_bar(dl, parent=self.mb, total=len(dl)):
            self.xb, self.yb = b
            self.one_batch()
            self.mb.child.comment = f"Loss: {self.loss.item()}"

    @with_cbs("batch")
    def one_batch(self):
        y_hat = self.model(*self.xb)
        self.loss = self.loss_fn(y_hat, self.yb)
        if self.model.training:
            self.loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
            self.train_step += 1

    def callback(self, nm):
        if self.cbs is not None:
            for cb in sorted(self.cbs):
                if hasattr(cb, nm): getattr(cb, nm)(self)

# %% ../nbs/03_training.ipynb 3
def default_device():
    is_cuda = torch.cuda.is_available()
    return torch.device("cuda" if is_cuda else "cpu")


class Callback:
    order = 0
    def __gt__(self, other): return self.order > other.order


class ToDeviceCB(Callback):
    def __init__(self, device=default_device()): self.device = device

    def before_fit(self, learn):
        self.model = learn.model.to(self.device)

    def before_batch(self, learn):
        learn.xb, learn.yb = learn.xb.to(self.device), learn.yb.to(self.device)


class FnCallback(Callback):
    order = 1
    def __init__(self, nm): self.nm = nm

    def __call__(self, fn):
        setattr(self, self.nm, fn)
        return self


class SaveModelCB(Callback):
    order = 1

    def __init__(self, save_dir, save_every_n=1000, max_to_keep=5):
        self.dir, self.save_every_n, self.max_to_keep = Path(save_dir), save_every_n, max_to_keep
        self.dir.mkdir(parents=True, exist_ok=True)

    def after_batch(self, learn):
        if learn.train_step % self.save_every_n == 0:
            torch.save(learn.model.state_dict(), self.dir / f"model_{learn.train_step}.pt")
        files = sorted(self.dir.glob("model_*.pt"), key=lambda x: int(x.stem.split("_")[1]), reverse=True)
        if len(files) > self.max_to_keep:
            for f in files[self.max_to_keep:]: f.unlink()

    def after_fit(self, learn):
        torch.save(learn.model.state_dict(), self.dir / f"model_last.pt")


class WandbCB(Callback):
    order = SaveModelCB.order + 10

    def __init__(self, config):
        try: import wandb; wandb.require("core")
        except ImportError: raise ImportError("Please install wandb to use this callback")
        self.wandb = wandb
        self.cfg = config
        self.wcfg = config.wandb

    def before_fit(self, learn):
        learn.stats = {}
        self.run = self.wandb.init(project=self.wcfg.project,
                                   name=self.wcfg.name,
                                   config=OmegaConf.to_container(self.cfg),
                                   mode="disabled" if self.cfg.debug_mode else "online")

    def after_batch(self, learn):
        if learn.train_step % self.wcfg.log_every_steps == 0:
            if learn.model.training:
                self.run.log({
                    "loss": learn.loss,
                    "lr": learn.optimizer.param_groups[0]['lr'],
                    **learn.stats
                }, step=learn.train_step)
            else:
                self.run.log({"valid_loss": learn.loss}, step=learn.train_step)

    def after_fit(self, learn):
        for cb in learn.cbs:
            if isinstance(cb, SaveModelCB):
                self.run.save(str(cb.dir / "model_last.pt"))
        self.run.finish()


def repr_cbs(cbs):
    return " ".join(cb.__class__.__name__ for cb in cbs)

# %% ../nbs/03_training.ipynb 5
class CurriculumCB(Callback):
    def __init__(self, curriculum_config):
        self.cfg = curriculum_config

    def get_params(self, name, step):
        cfg = self.cfg[name]
        return min(cfg.end, cfg.start + step // cfg.interval * cfg.inc)

    def update_task(self, learn, ds):
        ds.task.n_points = self.get_params("points", learn.train_step)
        n_dims = self.get_params("dims", learn.train_step)
        ds.task.truncated_dims = ds.task.n_dims - n_dims
        if hasattr(learn, 'stats'):
            learn.stats['n_points'] = ds.task.n_points
            learn.stats['n_dims'] = n_dims

    def after_batch(self, learn):
        if learn.model.training: self.update_task(learn, learn.dl_train.dataset)
        else: self.update_task(learn, learn.dl_valid.dataset)


class LoopCB(CurriculumCB):
    def update_model(self, learn):
        learn.model.n_loops = self.get_params("loops", learn.train_step)
        if hasattr(learn, 'stats'):
            learn.stats['n_loops'] = learn.model.n_loops

    def after_batch(self, learn): self.update_model(learn)
