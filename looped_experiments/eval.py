# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_eval.ipynb.

# %% auto 0
__all__ = ['default_fig_params', 'MetricsCB', 'squared_error', 'looped_error', 'eval', 'aggregate_metrics', 'plot_results']

# %% ../nbs/04_eval.ipynb 1
import logging as l
from pathlib import Path

import torch

from .all import *
import looped_experiments.wandb_utils as wu
import matplotlib.pyplot as plt
import numpy as np

# %% ../nbs/04_eval.ipynb 2
class MetricsCB(Callback):
    def __init__(self, metric_fn): self.metric_fn = metric_fn
    def before_fit(self, _): self.metrics = []

    def after_batch(self, learn):
        self.metrics.append(to_cpu(self.metric_fn(learn.preds, learn.yb)))

    def after_epoch(self, learn):
        try:
            self.metrics = torch.cat(self.metrics)
        except TypeError:
            l.warning("Was not able to concatenate metrics")
            pass

# %% ../nbs/04_eval.ipynb 3
def squared_error(preds, ys):
    return (preds - ys).square()


def looped_error(preds, ys):
    # for lost tokens we take predictions from last loop they appeared in
    res = torch.zeros_like(preds[0])
    for pred in preds:
        res[~pred.isnan()] = pred[~pred.isnan()]
    return (res - ys).square()

# %% ../nbs/04_eval.ipynb 4
def eval(cfg, model_path=None, metric_fn=None):
    train = cfg.training
    task = get_task_cls(cfg.task.task_name)(train.batch_size, **cfg.task)
    dl_eval = dataloader(task, train.eval_steps)

    @FnCallback("before_batch")
    def trans_input(learner): learner.xb = (learner.xb, learner.yb)

    model = get_model(cfg.model)
    weights = torch.load(model_path or cfg.out_dir / "model_last.pt", weights_only=True)
    if '+train' in str(model_path): weights = weights['model_state_dict']
    model.load_state_dict(weights)
    loss_fn = get_loss(cfg.model)
    metric_fn = metric_fn or (looped_error if 'loop' in cfg.model.family else squared_error)
    metric_cb = MetricsCB(metric_fn)
    cbs = [ToDeviceCB(), trans_input, metric_cb]
    Learner(model, None, dl_eval, None, loss_fn=loss_fn, cbs=cbs).eval()
    return metric_cb.metrics

# %% ../nbs/04_eval.ipynb 11
def aggregate_metrics(result_dict, non_truncated_dims, bootstrap_trials=1000):
    """
    Takes as input a tensor of shape (num_eval, n_points) and returns a dict with
    per-point mean, stddev, and bootstrap limits
    """
    d = non_truncated_dims
    results = {}
    for model_name in result_dict.keys():
        errs = result_dict[model_name]
        tmp = {}
        tmp["mean"] = errs.mean(0) / d
        tmp["std"] = errs.std(0, unbiased=True) / d
        n = len(errs)
        bootstrap_indices = torch.randint(n, size=(bootstrap_trials, n))
        bootstrap_means = errs[bootstrap_indices].mean(dim=1).sort(dim=0)[0]
        tmp["bootstrap_low"] = bootstrap_means[int(0.05 * bootstrap_trials), :] / d
        tmp["bootstrap_high"] = bootstrap_means[int(0.95 * bootstrap_trials), :] / d
        results[model_name] = tmp

    return results

# %% ../nbs/04_eval.ipynb 13
default_fig_params = {
    'figsize': (8, 5),
    'ticksize': 20,
    'linewidth': 5
}


def plot_results(res: dict, ax=None, fig_params=default_fig_params, cmap="coolwarm"):
    if ax is None: ax = plt.subplots(figsize=fig_params['figsize'])[1]
    colors = plt.get_cmap(cmap)(np.linspace(0, 1, len(res)))

    for col, (m_name, err) in zip(colors, res.items()):
        ax.plot(err["mean"], color=col, lw=fig_params['linewidth'], label=m_name.capitalize())
        ax.fill_between(range(len(err["mean"])),
                        err["bootstrap_low"],
                        err["bootstrap_high"],
                        alpha=0.3, color=col)

    ax.tick_params(axis='both', labelsize=fig_params['ticksize'])
    return ax
