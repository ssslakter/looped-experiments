wandb:
    project: looped-transformer
    name: null
    log_every_steps: 50

gpu:
    cuda: True

model:
    family: gpt2
    n_embd: 256
    n_layer: 6
    n_head: 8
    n_dims: 20
    n_positions: 101
    pred_type: regression
    # GPT-2 defaults
    dropout: 0.0
    bias: True


task_name: linear_regression
task:
    n_points: 30
    n_dims: 20
    std: 0.0
    sparsity: null

training:
    batch_size: 128
    n_epoch: 5
    train_steps: 10000
    eval_steps: 1000
    learning_rate: 0.0002
    weight_decay: 0.0
    save_every_steps: 1000
    # keep_every_steps: 10000
    curriculum:
        dims:
            start: 5
            end: 20
            inc: 1
            interval: 1000
        points:
            start: 11
            end: 41
            inc: 2
            interval: 1000
        loops:
            start: 1
            end: 1
            inc: 2
            interval: 100
    n_loop_window: 20

out_dir: ./results/linear_regression_baseline
debug_mode: False