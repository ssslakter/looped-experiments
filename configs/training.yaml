training:
  batch_size: 128
  n_epoch: 10 # for infinite datasets, used only for more frequent evaluation
  train_steps: 5000
  eval_steps: 1000
  learning_rate: 0.0002
  weight_decay: 0.0
  save_every_steps: 1000
  curriculum:
    dims:
        start: 5
        end: 20
        inc: 1
        interval: 3000
    points:
        start: 11
        end: 41
        inc: 2
        interval: 3000
    loops:
        start: 1
        end: 1
        inc: 2
        interval: 100
        n_loop_window: 20